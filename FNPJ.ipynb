{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DS5110 FNPJ\n",
    "## Protocol illustration\n",
    "### TCP:\n",
    "TCP States:\n",
    "\n",
    "SYN (Synchronization): Used to create a TCP connection\n",
    " \n",
    "ACK (Acknowledgement): Used to acknowledge the reception of data or synchronization packets\n",
    "\n",
    "PSH (Push): Instruct the network stacks to bypass buffering\n",
    "\n",
    "URG (Urgent): Indicates out-of-band data that must be processed by the network stacks before \n",
    "normal data\n",
    "\n",
    "FIN (Finish): Gracefully terminate the TCP connection\n",
    "\n",
    "RST (Reset): Immediately terminate the connection and drop any in-transit data\n",
    "## DDoS type and detection\n",
    "\n",
    "### TCP-based DDoS:\n",
    "#### SYN Flood: \n",
    "\n",
    "dur: < 0.1 sec (very short duration)  \n",
    "state: match the regex:`^(S|S_[ASR]|S_RA|S_SA)$`  \n",
    "tot_pkts/dur > normal baseline\n",
    "\n",
    "#### ACK Flood:  \n",
    "dur: > 10 sec (relatively long)  \n",
    "tot_bytes: < 100 or (< baseline)  \n",
    "state: match `^A+$|^[^APUFR]*A[^APUFR]*$`  \n",
    "\n",
    "#### RST Attack:  \n",
    "dur: < 0.1 or 0.05 sec (very short duration)  \n",
    "state: `'^R|.*_R.*$'`  \n",
    "tot_pkts/dur > normal baseline  \n",
    "\n",
    "#### State Exhaustion Attack:  \n",
    "dur: > 30 sec (relatively long)\n",
    "tot_pkts: <= 3 (rare)   \n",
    "state: `'^S[^A]*$|^S[^A]*_[^A]*$'`  \n",
    "\n",
    "### UDP-based DDoS:\n",
    "#### UDP Flood:  \n",
    "Massive UDP small packets  \n",
    "dur: < 0.1 sec  \n",
    "tot_pkts/dur > normal baseline \n",
    "tot_bytes: < 100 or (< baseline)  \n",
    "src_bytes / tot_bytes: < 0.1 (close to 0)  \n",
    "\n",
    "### ICMP-based DDoS:\n",
    "\n",
    "#### ICMP Flood:  \n",
    "Massive ICMP packets with high packet rate and uniform packet size  \n",
    "dir: ->  \n",
    "tot_pkts/dur > normal baseline (could be 1000)  \n",
    "tot_bytes / dur < 200  \n",
    "state: mainly ECO\n",
    "\n",
    "#### Ping of Death:  \n",
    "Extremely large paket  \n",
    "dir: ->  \n",
    "bytes_per_pkt > 2000 or  \n",
    "tot_bytes: > 65535 bytes or  \n",
    "tot_pkts>10 & bytes_per_pkt>1000  \n",
    "state: ECO  \n",
    "\n",
    "#### Smurf Attack: \n",
    "dir: <-  \n",
    "bytes_per_pkt<200  \n",
    "state: ECR  \n",
    "search for an identical but opposite direction flow (same dur, tot_pkts, tot_bytes, state == ECO \n",
    "and src_bytes == tot_bytes)\n",
    "\n",
    "### HTTP-based DDoS: \n",
    "#### HTTP Flood:  \n",
    "tot_pkts/dur > normal baseline (could be 1000)  \n",
    "tot_bytes < 100 (normal requests usually are 200 bytes+)  \n",
    "\n",
    "#### Slowloris:  \n",
    "dur > 60 sec (usually < 10 sec)  \n",
    "tot_pkts <= 3 (rare number)  \n",
    "src_bytes < 100 (originator sent few contents)\n",
    "\n",
    "\n",
    "#### RUDY Attack:  \n",
    "dur > 300 sec (extremely long connection)\n",
    "src_bytes/dur < 1 (extremely low transmission speed) \n",
    "\n",
    "### ARP-based DDoS:\n",
    "#### ARP Flood:\n",
    "tot_pkts/dur > 500 (usually 10)\n",
    "tot_bytes $\\approx$ 60  \n",
    "state == 'REQ' or 'RSP'\n",
    "  \n",
    "#### ARP Spoofing:    \n",
    "Without IP and MAC, ARP Spoofing is hard to detect and lack accuracy and preciseness\n",
    "tot_bytes > 70 or < 50 (usually 60)  \n",
    "state == 'RSP'\n",
    "\n",
    "### RTP/UDT-based DDoS:\n",
    "#### Media Flow Flooding:  \n",
    "Single direction, high packet rate, bandwidth occupation, and abnormal source flow\n",
    "tot_pkts/dur > 5000 or baseline  \n",
    "tot_bytes/dur > 100 mbps (1e8 bytes)  \n",
    "src_bytes/tot_bytes < 0.1  \n",
    "dir == '<-' or '->'  \n",
    "\n",
    " \n",
    "\"\"\""
   ],
   "id": "b5d1d78d3a23c665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:46:34.701686Z",
     "start_time": "2025-04-16T20:46:31.753207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from typing import Tuple, Dict\n",
    "from enum import Enum, auto\n",
    "from re import compile\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, silhouette_score, adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data load and preprocessing here\n",
    "file_path = '1-Neris-20110810.binetflow.parquet'\n",
    "df_primitive = pd.read_parquet(file_path)\n",
    "\n",
    "Norm_reg = compile(r'(?i)\\b(normal)\\b')\n",
    "Norm_Bckgnd_reg = compile(r'(?i)\\b(background|normal)\\b')\n",
    "\n",
    "\n",
    "class TCPState(Enum):\n",
    "    # An enum that represents the TCP states, auto() keep these states ordered\n",
    "    START = auto()\n",
    "    SYN_SENT = auto()\n",
    "    SYN_ACKED = auto()\n",
    "    ACKED = auto()\n",
    "    RESET = auto()\n",
    "    FINISHED = auto()\n",
    "\n",
    "def is_abnormal_fsm(tcp_states: str) -> (bool, str):\n",
    "    \"\"\"\n",
    "    Determine if a TCP state is abnormal.\n",
    "    :param tcp_states: A string which the case doesn't matter that represents a TCP handshake \n",
    "    :return: True if the state is abnormal, else False\n",
    "    \"\"\"\n",
    "    current_state = TCPState.START\n",
    "    for char in tcp_states.replace('_', ''):  # Remove delimiters\n",
    "        if char == 'S':\n",
    "            if current_state != TCPState.START:\n",
    "                return True, f\"Unexpected SYN in state {current_state}\"\n",
    "            current_state = TCPState.SYN_SENT\n",
    "        elif char == 'A':\n",
    "            if current_state == TCPState.SYN_SENT:\n",
    "                current_state = TCPState.SYN_ACKED\n",
    "            elif current_state == TCPState.SYN_ACKED:\n",
    "                current_state = TCPState.ACKED\n",
    "            else:\n",
    "                return True, f\"Unexpected ACK in state {current_state}\"\n",
    "        elif char == 'R':\n",
    "            current_state = TCPState.RESET\n",
    "            break  # RST terminates the flow\n",
    "        elif char == 'F':\n",
    "            current_state = TCPState.FINISHED\n",
    "        else:\n",
    "            return True, f\"Invalid flag '{char}'\"\n",
    "\n",
    "    # Check if the final state is valid\n",
    "    if current_state not in {TCPState.ACKED, TCPState.FINISHED, TCPState.RESET}:\n",
    "        return True, f\"Incomplete handshake (ended in {current_state})\"\n",
    "    return False, \"Normal flow\"\n",
    "\n",
    "\n",
    "def preprocess(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataframe, add two columns for convenience, and prepare two dataframe for \n",
    "    Normal flow only and Normal/Background flow only.\n",
    "    :param df_: A DataFrame to be preprocessed\n",
    "    :return: A preprocessed DataFrame added is_ddos and ddos_type columns\n",
    "    \"\"\"\n",
    "    df_ = df_.iloc[:, :-1]\n",
    "    # Rows containing NaN have to be gone\n",
    "    for column in df_primitive.columns:\n",
    "        df_primitive.drop(df_primitive[df_primitive[column].isin([np.nan])].index, inplace=True)\n",
    "\n",
    "    df_['is_ddos'] = 0  \n",
    "    df_['ddos_type'] = 'normal'\n",
    "    \n",
    "    return df_\n",
    "\n",
    "# Perform preprocessing and create baselines with normal netflow and possible needed \n",
    "# normal/background netflow\n",
    "df_primitive = preprocess(df_primitive)\n",
    "df_norm = df_primitive[df_primitive['label'].str.contains(Norm_reg, regex=True, na=False)]\n",
    "df_norm_bkgnd = df_primitive[df_primitive['label'].str.contains(Norm_Bckgnd_reg, regex=True, na=False)]\n",
    "# print(df_norm.head(5), \"\\n\", \"=\" * 50)\n",
    "# print(df_norm_bkgnd.head(5))\n",
    "\n",
    "\n",
    "def calculate_baseline(df_):\n",
    "    df = df_.copy()\n",
    "    df['pkts_per_sec'] = np.where(df['dur'] > 0,\n",
    "                                  df['tot_pkts'] / df['dur'],\n",
    "                                  np.nan)  # 将dur=0的速率设为NaN\n",
    "    df['bytes_per_sec'] = np.where(df['dur'] > 0,\n",
    "                                   df['tot_bytes'] / df['dur'],\n",
    "                                   np.nan)\n",
    "    df['src_bytes_per_sec'] = np.where(df['dur'] > 0,\n",
    "                                       df['src_bytes'] / df['dur'],\n",
    "                                       np.nan)\n",
    "\n",
    "    # overall stat\n",
    "    stats = {\n",
    "        'global': {\n",
    "            # basic metrics\n",
    "            'dur_median': df['dur'].median(),\n",
    "            'dur_q90': df['dur'].quantile(0.90),\n",
    "            'tot_pkts_median': df['tot_pkts'].median(),\n",
    "            'tot_bytes_q90': df['tot_bytes'].quantile(0.9),\n",
    "            'src_bytes_median': df['src_bytes'].median(),\n",
    "\n",
    "            # rate metrics\n",
    "            'pkts_per_sec_median': df['pkts_per_sec'].median(skipna=True),\n",
    "            'pkts_per_sec_q90': df['pkts_per_sec'].quantile(0.9, interpolation='higher'),\n",
    "            'bytes_per_sec_median': df['bytes_per_sec'].median(skipna=True),\n",
    "            'bytes_per_sec_q90': df['bytes_per_sec'].quantile(0.9, interpolation='higher'),\n",
    "            'src_bytes_per_sec_median': df['src_bytes_per_sec'].median(skipna=True),\n",
    "            'src_bytes_per_sec_q90': df['src_bytes_per_sec'].quantile(0.9, interpolation='higher')\n",
    "        },\n",
    "\n",
    "        # stat based on protocol\n",
    "        'by_proto': df.groupby('proto').agg({\n",
    "            # basic metrics\n",
    "            'tot_pkts': [('median', 'median'),\n",
    "                         ('q90', lambda x: x.quantile(0.9))],\n",
    "            'tot_bytes': [('median', 'median'),\n",
    "                          ('q90', lambda x: x.quantile(0.9))],\n",
    "            'src_bytes': [('median', 'median'),\n",
    "                          ('q90', lambda x: x.quantile(0.9))],\n",
    "            'dur': [('median', 'median'),\n",
    "                    ('count', 'count')],\n",
    "\n",
    "            # flow/pkt rate metrics\n",
    "            'pkts_per_sec': [('median', lambda x: x.median(skipna=True)),\n",
    "                             ('q90', lambda x: x.quantile(0.9, interpolation='higher'))],\n",
    "            'bytes_per_sec': [('median', lambda x: x.median(skipna=True)),\n",
    "                              ('q90', lambda x: x.quantile(0.9, interpolation='higher'))],\n",
    "            'src_bytes_per_sec': [('median', lambda x: x.median(skipna=True)),\n",
    "                                  ('q90', lambda x: x.quantile(0.9, interpolation='higher'))]\n",
    "        })\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "norm_baseline = calculate_baseline(df_norm)\n",
    "norm_bkgnd_baseline = calculate_baseline(df_norm_bkgnd)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIONE\\AppData\\Local\\Temp\\ipykernel_42536\\2099961564.py:88: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_norm = df_primitive[df_primitive['label'].str.contains(Norm_reg, regex=True, na=False)]\n",
      "C:\\Users\\DIONE\\AppData\\Local\\Temp\\ipykernel_42536\\2099961564.py:89: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_norm_bkgnd = df_primitive[df_primitive['label'].str.contains(Norm_Bckgnd_reg, regex=True, na=False)]\n",
      "C:\\Users\\DIONE\\AppData\\Local\\Temp\\ipykernel_42536\\2099961564.py:126: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  'by_proto': df.groupby('proto').agg({\n",
      "C:\\Users\\DIONE\\AppData\\Local\\Temp\\ipykernel_42536\\2099961564.py:126: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  'by_proto': df.groupby('proto').agg({\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:56:17.119534Z",
     "start_time": "2025-04-16T20:46:38.488033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DDoS detection part\n",
    "# print(norm_baseline)\n",
    "print(norm_baseline['global'])\n",
    "print(norm_baseline['by_proto'])\n",
    "print(norm_baseline.keys())\n",
    "print(norm_baseline['by_proto'].loc['tcp',('pkts_per_sec', 'median')])\n",
    "\n",
    "global_dur_baseline = norm_baseline['global']['dur_median']\n",
    "print(global_dur_baseline)\n",
    "\n",
    "\n",
    "class DDoSDetector:\n",
    "    def __init__(self, baseline: Dict, log_file: str = \"ddos_detection.log\"):\n",
    "        \"\"\"\n",
    "        Initialize detector with baseline metrics and logging\n",
    "        \n",
    "        :param baseline: Pre-calculated baseline metrics dictionary\n",
    "        :param log_file: Path to log file (default: ddos_detection.log)\n",
    "        \"\"\"\n",
    "        self.baseline = baseline\n",
    "        self._setup_logging(log_file)\n",
    "        self._init_regex()\n",
    "        self._load_baselines()\n",
    "\n",
    "    def _setup_logging(self, log_file: str) -> None:\n",
    "        \"\"\"Configure logging system\"\"\"\n",
    "        self.logger = logging.getLogger(\"DDoS_Detector\")\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        self.logger.info(\"DDoS Detector initialized\")\n",
    "\n",
    "    def _init_regex(self) -> None:\n",
    "        \"\"\"Compile attack pattern regexes\"\"\"\n",
    "        self.SYN_Flood = compile(r'^(S|S_[ASR]|S_RA|S_SA)$')\n",
    "        self.ACK_Flood = compile(r'^A+$|^[^APUFR]*A[^APUFR]*$')\n",
    "        self.RST_Attack = compile(r'^R|.*_R.*$')\n",
    "        self.Exhaustion_Attack = compile(r'^S[^A]*$|^S[^A]*_[^A]*$')\n",
    "\n",
    "    def _load_baselines(self) -> None:\n",
    "        \"\"\"Load protocol-specific baselines\"\"\"\n",
    "        self.baselines = {\n",
    "            'tcp': {\n",
    "                'pkt_rate': self.baseline['by_proto'].loc['tcp', ('pkts_per_sec', 'median')],\n",
    "                'byte_rate': self.baseline['by_proto'].loc['tcp', ('bytes_per_sec', 'median')]\n",
    "            },\n",
    "            'udp': {\n",
    "                'pkt_rate': self.baseline['by_proto'].loc['udp', ('pkts_per_sec', 'median')],\n",
    "                'byte_rate': self.baseline['by_proto'].loc['udp', ('bytes_per_sec', 'median')]\n",
    "            },\n",
    "            'icmp': {\n",
    "                'pkt_rate': self.baseline['by_proto'].loc['icmp', ('pkts_per_sec', 'median')],\n",
    "                'byte_rate': self.baseline['by_proto'].loc['icmp', ('bytes_per_sec', 'median')]\n",
    "            },\n",
    "            'media': {\n",
    "                proto: self.baseline['by_proto'].loc[proto, ('pkts_per_sec', 'median')]\n",
    "                for proto in ['udt', 'rtp']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def detect(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main detection function\n",
    "        \n",
    "        :param df: Input DataFrame with network flow data\n",
    "        :return: DataFrame with detection results\n",
    "        \"\"\"\n",
    "        if not {'proto', 'dur', 'tot_pkts', 'tot_bytes', 'state'}.issubset(df.columns):\n",
    "            raise ValueError(\"Missing required columns in DataFrame\")\n",
    "\n",
    "        # Initialize result columns\n",
    "        if 'is_ddos' not in df.columns:\n",
    "            df['is_ddos'] = 0\n",
    "        if 'ddos_type' not in df.columns:\n",
    "            df['ddos_type'] = 'Normal'\n",
    "\n",
    "        attack_counts = {}\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                proto = row['proto']\n",
    "                result = (0, 'Normal')\n",
    "\n",
    "                if proto == 'tcp':\n",
    "                    result = self._detect_tcp(\n",
    "                        row['dur'], row['tot_pkts'], row['tot_bytes'], row['state'])\n",
    "                elif proto == 'udp':\n",
    "                    result = self._detect_udp(\n",
    "                        row['dur'], row['tot_pkts'], row['tot_bytes'], row['src_bytes'])\n",
    "                elif proto == 'icmp':\n",
    "                    result = self._detect_icmp(\n",
    "                        row['dur'], row['dir'], row['tot_pkts'], row['tot_bytes'],\n",
    "                        row['state'], df[df['proto'] == 'icmp'])\n",
    "                elif proto == 'arp':\n",
    "                    result = self._detect_arp(\n",
    "                        row['dur'], row['tot_pkts'], row['tot_bytes'], row['state'])\n",
    "                elif proto in ['udt', 'rtp']:\n",
    "                    result = self._detect_media(\n",
    "                        row['dur'], row['dir'], row['tot_pkts'], row['tot_bytes'],\n",
    "                        row['src_bytes'])\n",
    "\n",
    "                # Update results\n",
    "                if result[0] != row['is_ddos'] or result[1] != row['ddos_type']:\n",
    "                    df.at[idx, 'is_ddos'], df.at[idx, 'ddos_type'] = result\n",
    "                    self.logger.info(\n",
    "                        f\"Row {idx} - {proto}: {row['state']} -> {result[1]}\")\n",
    "\n",
    "                    if result[0] == 1:\n",
    "                        attack_counts[result[1]] = attack_counts.get(result[1], 0) + 1\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Row {idx} processing failed: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Log summary\n",
    "        self._log_summary(attack_counts, df)\n",
    "        return df\n",
    "\n",
    "    def _detect_tcp(self, dur: float, pkts: int, bytes_: int, state: str) -> Tuple[int, str]:\n",
    "        \"\"\"TCP attack detection logic\"\"\"\n",
    "        rate = pkts / dur if dur > 0 else 0\n",
    "\n",
    "        if dur < 0.1 and self.SYN_Flood.match(state) and rate > self.baselines['tcp']['pkt_rate']:\n",
    "            return 1, \"TCP SYN Flood\"\n",
    "        if (dur > 10 and self.ACK_Flood.search(state) and\n",
    "                bytes_ < self.baselines['tcp']['byte_rate'] and\n",
    "                rate > self.baselines['tcp']['pkt_rate']):\n",
    "            return 1, \"TCP ACK Flood\"\n",
    "        if dur < 0.1 and self.RST_Attack.match(state) and rate > self.baselines['tcp']['pkt_rate']:\n",
    "            return 1, \"TCP RST Flood\"\n",
    "        if dur > 30 and self.Exhaustion_Attack.match(state) and pkts <= 3:\n",
    "            return 1, \"TCP Exhaustion Attack\"\n",
    "        return 0, \"Normal\"\n",
    "\n",
    "    def _detect_udp(self, dur: float, pkts: int, bytes_: int, src_bytes: int) -> Tuple[int, str]:\n",
    "        \"\"\"UDP attack detection logic\"\"\"\n",
    "        if dur <= 0:\n",
    "            return 0, \"Normal\"\n",
    "\n",
    "        if (pkts/dur > self.baselines['udp']['pkt_rate'] and\n",
    "                bytes_ < self.baselines['udp']['byte_rate'] and\n",
    "                src_bytes/bytes_ < 0.1):\n",
    "            return 1, \"UDP Flood\"\n",
    "        return 0, \"Normal\"\n",
    "\n",
    "    def _detect_icmp(self, dur: float, drctn: str, pkts: int, bytes_: int,\n",
    "                     state: str, icmp_df: pd.DataFrame) -> Tuple[int, str]:\n",
    "        \"\"\"ICMP attack detection logic\"\"\"\n",
    "        if dur <= 0 or pkts <= 0:\n",
    "            return 0, \"Normal\"\n",
    "\n",
    "        rate = pkts / dur\n",
    "        byte_rate = bytes_ / dur\n",
    "        bytes_per_pkt = bytes_ / pkts\n",
    "\n",
    "        # ICMP symmetry check\n",
    "        def _check_symmetry(dur_, pkts_, bytes__):\n",
    "            eco_mask = (\n",
    "                    (icmp_df['dur'] == dur_) &\n",
    "                    (icmp_df['dir'] == '->') &\n",
    "                    (icmp_df['tot_pkts'] == pkts_) &\n",
    "                    (icmp_df['tot_bytes'] == bytes__) &\n",
    "                    (icmp_df['state'] == \"ECO\") &\n",
    "                    (icmp_df['src_bytes'] == bytes__)\n",
    "            )\n",
    "            ecr_mask = (\n",
    "                    (icmp_df['dur'] == dur_) &\n",
    "                    (icmp_df['dir'] == '<-') &\n",
    "                    (icmp_df['tot_pkts'] == pkts_) &\n",
    "                    (icmp_df['tot_bytes'] == bytes__) &\n",
    "                    (icmp_df['state'] == \"ECR\") &\n",
    "                    (icmp_df['src_bytes'] == 0)\n",
    "            )\n",
    "            return not icmp_df[eco_mask].empty and not icmp_df[ecr_mask].empty\n",
    "\n",
    "        if (drctn == \"->\" and rate > self.baselines['icmp']['pkt_rate'] and\n",
    "                byte_rate < 200 and state == \"ECO\"):\n",
    "            return 1, \"ICMP Flood\"\n",
    "        if (drctn == \"->\" and state == \"ECO\" and\n",
    "                (bytes_per_pkt > 2000 or bytes_ > 65535 or (pkts > 10 and bytes_per_pkt > 1000))):\n",
    "            return 1, \"Ping of Death\"\n",
    "        if (drctn == \"<-\" and state == \"ECR\" and bytes_per_pkt < 200 and\n",
    "                _check_symmetry(dur, pkts, bytes_)):\n",
    "            return 1, \"Smurf Attack\"\n",
    "        return 0, \"Normal\"\n",
    "\n",
    "    def _detect_arp(self, dur: float, pkts: int, bytes_: int, state: str) -> Tuple[int, str]:\n",
    "        \"\"\"ARP attack detection logic\"\"\"\n",
    "        if dur <= 0:\n",
    "            return 0, \"Normal\"\n",
    "\n",
    "        if pkts/dur > 500 and 55 <= bytes_ <= 65 and state in ('REQ', 'RSP'):\n",
    "            return 1, \"ARP Flood\"\n",
    "        return 0, \"Normal\"\n",
    "\n",
    "    def _detect_media(self, dur: float, drctn: str, pkts: int, bytes_: int,\n",
    "                      src_bytes: int) -> Tuple[int, str]:\n",
    "        \"\"\"UDT/RTP media flood detection\"\"\"\n",
    "        if dur <= 0:\n",
    "            return 0, \"Normal\"\n",
    "\n",
    "        if (pkts/dur > 5000 and bytes_/dur > 1e8 and\n",
    "                src_bytes/bytes_ < 0.1 and drctn in ('<-', '->')):\n",
    "            return 1, \"Media Flow Flood\"\n",
    "        return 0, \"Normal\"\n",
    "\n",
    "    def _log_summary(self, attack_counts: Dict[str, int], df: pd.DataFrame) -> None:\n",
    "        \"\"\"Log detection summary statistics\"\"\"\n",
    "        total = sum(attack_counts.values())\n",
    "        self.logger.info(\"\\n==== Detection Summary ====\")\n",
    "        self.logger.info(f\"Total flows processed: {len(df)}\")\n",
    "        self.logger.info(f\"Total attacks detected: {total}\")\n",
    "\n",
    "        for attack, count in attack_counts.items():\n",
    "            self.logger.info(f\"{attack}: {count} cases\")\n",
    "\n",
    "\n",
    "detector = DDoSDetector(norm_baseline, \"network_security.log\")\n",
    "result_df = detector.detect(df_primitive)\n"
   ],
   "id": "9a0732654b2fe416",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dur_median': 0.0063795000314712524, 'dur_q90': 66.5772605895996, 'tot_pkts_median': 2.0, 'tot_bytes_q90': 5427.8000000000175, 'src_bytes_median': 81.0, 'pkts_per_sec_median': 750.2032582888328, 'pkts_per_sec_q90': 8298.755133293731, 'bytes_per_sec_median': 98217.18495195074, 'bytes_per_sec_q90': 1124087.6285926236, 'src_bytes_per_sec_median': 30511.093395889497, 'src_bytes_per_sec_q90': 321266.97591282654}\n",
      "          tot_pkts       tot_bytes          src_bytes                  dur  \\\n",
      "            median   q90    median      q90    median     q90       median   \n",
      "proto                                                                        \n",
      "arp            3.0   8.0     180.0    480.0      60.0   240.0  1800.366211   \n",
      "esp            NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "icmp           1.0   1.8     137.0    795.4     137.0   795.4     0.000000   \n",
      "igmp           NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "ipv6           NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "ipv6-icmp      NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "ipx/spx        NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "pim            NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "rarp           NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "rtcp           NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "rtp            NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "tcp           11.0  59.0    2044.0  33077.8     988.0  5773.0     1.090695   \n",
      "udp            2.0   2.0     244.0    402.0      76.0    85.0     0.000374   \n",
      "udt            NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "unas           NaN   NaN       NaN      NaN       NaN     NaN          NaN   \n",
      "\n",
      "                 pkts_per_sec               bytes_per_sec                \\\n",
      "           count       median          q90         median           q90   \n",
      "proto                                                                     \n",
      "arp           13     0.002262  3976.143233       0.135731  2.385686e+05   \n",
      "esp            0          NaN          NaN            NaN           NaN   \n",
      "icmp           3    25.731085    25.731085   12350.920937  1.235092e+04   \n",
      "igmp           0          NaN          NaN            NaN           NaN   \n",
      "ipv6           0          NaN          NaN            NaN           NaN   \n",
      "ipv6-icmp      0          NaN          NaN            NaN           NaN   \n",
      "ipx/spx        0          NaN          NaN            NaN           NaN   \n",
      "pim            0          NaN          NaN            NaN           NaN   \n",
      "rarp           0          NaN          NaN            NaN           NaN   \n",
      "rtcp           0          NaN          NaN            NaN           NaN   \n",
      "rtp            0          NaN          NaN            NaN           NaN   \n",
      "tcp         8525    17.326324   382.722799    4269.529678  4.101796e+04   \n",
      "udp        15061  5347.593645  9259.259244  718034.761862  1.248980e+06   \n",
      "udt            0          NaN          NaN            NaN           NaN   \n",
      "unas           0          NaN          NaN            NaN           NaN   \n",
      "\n",
      "          src_bytes_per_sec                 \n",
      "                     median            q90  \n",
      "proto                                       \n",
      "arp                0.067865  119284.296989  \n",
      "esp                     NaN            NaN  \n",
      "icmp           12350.920937   12350.920937  \n",
      "igmp                    NaN            NaN  \n",
      "ipv6                    NaN            NaN  \n",
      "ipv6-icmp               NaN            NaN  \n",
      "ipx/spx                 NaN            NaN  \n",
      "pim                     NaN            NaN  \n",
      "rarp                    NaN            NaN  \n",
      "rtcp                    NaN            NaN  \n",
      "rtp                     NaN            NaN  \n",
      "tcp             1455.325419   15258.610822  \n",
      "udp           203804.343304  358078.606546  \n",
      "udt                     NaN            NaN  \n",
      "unas                    NaN            NaN  \n",
      "dict_keys(['global', 'by_proto'])\n",
      "17.326323562652448\n",
      "0.0063795000314712524\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T20:59:06.977571Z",
     "start_time": "2025-04-16T20:59:03.275621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_copy = df_primitive.copy()\n",
    "\n",
    "\n",
    "\n",
    "y_bin   = df_copy[\"is_ddos\"]           # 0 / 1\n",
    "y_multi = df_copy[\"ddos_type\"]         # \"Normal\" or specific attack name\n",
    "X_raw       = df_copy.drop(columns=[\"state\", \"label\", \"is_ddos\", \"ddos_type\"])\n",
    "\n",
    "num_cols = X_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,      # keep it sparse if >30% zeros\n",
    ")\n",
    "\n",
    "X_scaled = preprocess.fit_transform(X_raw)\n",
    "\n",
    "(\n",
    "    X_tmp,\n",
    "    X_test,\n",
    "    y_tmp_bin,\n",
    "    y_test_bin,\n",
    "    y_tmp_multi,\n",
    "    y_test_multi,\n",
    ") = train_test_split(\n",
    "    X_scaled,\n",
    "    y_bin,\n",
    "    y_multi,\n",
    "    test_size=0.20,\n",
    "    stratify=y_bin,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train_bin,\n",
    "    y_val_bin,\n",
    "    y_train_multi,\n",
    "    y_val_multi,\n",
    ") = train_test_split(\n",
    "    X_tmp,\n",
    "    y_tmp_bin,\n",
    "    y_tmp_multi,\n",
    "    test_size=0.25,          # 0.25 × 0.80  ≈  0.20 of full set\n",
    "    stratify=y_tmp_bin,\n",
    "    random_state=42,\n",
    ")\n"
   ],
   "id": "9297a8596af0e93f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-16T21:31:43.611887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training\n",
    "print(\"\\n===  K‑MEANS (2 clusters) — Normal vs Attack  ===\")\n",
    "km_bin = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "km_bin.fit(X_train)\n",
    "\n",
    "def majority_map(preds, true_labels):\n",
    "    \"\"\"Map each cluster id to the majority class it captured.\"\"\"\n",
    "    mapping = {}\n",
    "    for cid in np.unique(preds):\n",
    "        maj = true_labels[preds == cid].value_counts().idxmax()\n",
    "        mapping[cid] = maj\n",
    "    return mapping\n",
    "\n",
    "cluster_map = majority_map(km_bin.labels_, y_train_bin.reset_index(drop=True))\n",
    "val_preds   = [cluster_map[c] for c in km_bin.predict(X_val)]\n",
    "print(classification_report(y_val_bin, val_preds, digits=4))\n",
    "print(\"Silhouette (train):\", silhouette_score(X_train, km_bin.labels_).round(4))\n",
    "\n",
    "# ----- visualisation\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_vis = pca.fit_transform(X_test)\n",
    "test_clusters = km_bin.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_vis[:, 0], X_vis[:, 1], s=5, c=test_clusters)\n",
    "plt.title(\"K‑Means (2 clusters) — test set\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# 3‑B.  Normal vs Attack  —  DBSCAN\n",
    "\n",
    "print(\"\\n===  DBSCAN (Normal vs Attack)  ===\")\n",
    "db = DBSCAN(eps=0.5, min_samples=10).fit(X_train)\n",
    "core_mask = db.labels_ != -1\n",
    "if core_mask.any():\n",
    "    print(\"Silhouette (core points):\",\n",
    "          silhouette_score(X_train[core_mask], db.labels_[core_mask]).round(4))\n",
    "else:\n",
    "    print(\"All points marked noise — adjust eps/min_samples!\")\n",
    "\n",
    "# ----- visualisation\n",
    "db_test = DBSCAN(eps=0.5, min_samples=10).fit(X_test)\n",
    "X_vis2 = PCA(n_components=2, random_state=42).fit_transform(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_vis2[:, 0], X_vis2[:, 1], s=5, c=db_test.labels_)\n",
    "plt.title(\"DBSCAN — test set\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4.  Cluster  DDoS  flows by attack type\n",
    "\n",
    "ddos_mask   = y_bin == 1\n",
    "X_ddos      = X_scaled[ddos_mask]\n",
    "y_ddos      = y_multi[ddos_mask]\n",
    "n_types     = y_ddos.nunique()\n",
    "\n",
    "print(f\"\\n===  K‑MEANS ({n_types} clusters) — within DDoS flows  ===\")\n",
    "km_ddos = KMeans(n_clusters=n_types, n_init=20, random_state=42).fit(X_ddos)\n",
    "print(\"Adjusted Rand Index:\", adjusted_rand_score(y_ddos, km_ddos.labels_).round(4))\n",
    "\n",
    "# ----- visualisation\n",
    "X_vis3 = pca.fit_transform(X_ddos.toarray() if hasattr(X_ddos, \"toarray\") else X_ddos)\n",
    "plt.figure()\n",
    "plt.scatter(X_vis3[:, 0], X_vis3[:, 1], s=5, c=km_ddos.labels_)\n",
    "plt.title(\"K‑Means clustering inside DDoS flows\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ],
   "id": "c2dc1f967dde9b8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===  K‑MEANS (2 clusters) — Normal vs Attack  ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\HW\\DS 5110\\DS5110_VENV\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\HW\\DS 5110\\DS5110_VENV\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\HW\\DS 5110\\DS5110_VENV\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9972    1.0000    0.9986    323340\n",
      "           1     0.0000    0.0000    0.0000       895\n",
      "\n",
      "    accuracy                         0.9972    324235\n",
      "   macro avg     0.4986    0.5000    0.4993    324235\n",
      "weighted avg     0.9945    0.9972    0.9959    324235\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import shap\n",
    "\n",
    "print(\"\\n===  Surrogate GB classifier for SHAP explanations  ===\")\n",
    "y_train_sur  = km_bin.predict(X_train)\n",
    "y_val_sur    = km_bin.predict(X_val)\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "gb.fit(X_train, y_train_sur)\n",
    "print(\"Surrogate accuracy (val):\", gb.score(X_val, y_val_sur).round(4))\n",
    "\n",
    "explainer = shap.TreeExplainer(\n",
    "    gb,\n",
    "    feature_names=preprocess.get_feature_names_out(),\n",
    "    feature_perturbation=\"tree_path_dependent\",\n",
    ")\n",
    "# smaller background sample for speed\n",
    "sample = shap.utils.sample(X_val, 2000, random_state=1)\n",
    "shap_values = explainer.shap_values(sample, check_additivity=False)\n",
    "\n",
    "#── global explanation\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    sample,\n",
    "    feature_names=preprocess.get_feature_names_out(),\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "#── local explanation (pick any row)\n",
    "idx = 0\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    explainer.shap_values(X_val[idx])[0],\n",
    "    features=X_val[idx],\n",
    "    feature_names=preprocess.get_feature_names_out(),\n",
    ")"
   ],
   "id": "5c2efb4eb4c307d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
